{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Experiments and Extensions\n",
    "\n",
    "This notebook demonstrates some advanced experiments and extension features, including:\n",
    "\n",
    "1. Continuous Hopfield Network\n",
    "2. Deep Boltzmann Machine\n",
    "3. Conditional Generation\n",
    "4. Transfer Learning\n",
    "\n",
    "These experiments show the flexibility and powerful functions of energy models and generative models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project root directory to path\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))))\n",
    "\n",
    "# Import project modules\n",
    "from src.hopfield.hopfield import HopfieldNetwork\n",
    "from src.hopfield.visualizer import HopfieldVisualizer\n",
    "from src.boltzmann.rbm import RBM\n",
    "from src.boltzmann.sampler import RBMSampler\n",
    "from src.utils.config import Config\n",
    "from src.utils.data_loader import load_mnist, load_fashion_mnist\n",
    "from src.utils.helpers import binary_to_image, image_to_binary\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Load configuration\n",
    "config = Config()\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() and config.RBM_CONFIG['use_cuda'] else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Continuous Hopfield Network\n",
    "\n",
    "The continuous Hopfield network uses continuous dynamics equations instead of discrete updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousHopfieldNetwork:\n",
    "    \"\"\"\n",
    "    Continuous Hopfield Network\n",
    "    \"\"\"\n",
    "    def __init__(self, n_neurons, tau=1.0, dt=0.1):\n",
    "        self.n_neurons = n_neurons\n",
    "        self.tau = tau  # Time constant\n",
    "        self.dt = dt   # Time step\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.W = torch.zeros(n_neurons, n_neurons)\n",
    "        \n",
    "        # Initialize states\n",
    "        self.states = torch.zeros(n_neurons)\n",
    "    \n",
    "    def store_patterns(self, patterns):\n",
    "        \"\"\"\n",
    "        Store patterns using Hebbian learning rule\n",
    "        \"\"\"\n",
    "        # Convert patterns to tensors\n",
    "        if not isinstance(patterns, torch.Tensor):\n",
    "            patterns = torch.tensor(patterns, dtype=torch.float32)\n",
    "        \n",
    "        # Hebbian learning rule\n",
    "        self.W = torch.zeros(self.n_neurons, self.n_neurons)\n",
    "        for p in patterns:\n",
    "            self.W += torch.outer(p, p)\n",
    "        \n",
    "        # Remove diagonal elements\n",
    "        self.W.fill_diagonal_(0)\n",
    "        \n",
    "        # Normalize weights\n",
    "        self.W /= self.n_neurons\n",
    "    \n",
    "    def energy(self, state):\n",
    "        \"\"\"\n",
    "        Calculate energy function\n",
    "        \"\"\"\n",
    "        return -0.5 * torch.dot(state, torch.mv(self.W, state))\n",
    "    \n",
    "    def dynamics(self, state):\n",
    "        \"\"\"\n",
    "        Calculate dynamics\n",
    "        \"\"\"\n",
    "        return (-state + torch.mv(self.W, state)) / self.tau\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        Update network state\n",
    "        \"\"\"\n",
    "        # Euler method\n",
    "        self.states += self.dt * self.dynamics(self.states)\n",
    "    \n",
    "    def recover(self, initial_state, n_steps=100):\n",
    "        \"\"\"\n",
    "        Recover pattern from initial state\n",
    "        \"\"\"\n",
    "        # Set initial state\n",
    "        self.states = initial_state.clone()\n",
    "        \n",
    "        # Record energy changes\n",
    "        energies = [self.energy(self.states).item()]\n",
    "        \n",
    "        # Update network\n",
    "        for _ in range(n_steps):\n",
    "            self.update()\n",
    "            energies.append(self.energy(self.states).item())\n",
    "        \n",
    "        return self.states.clone(), energies\n",
    "\n",
    "# Create continuous Hopfield network\n",
    "continuous_hopfield = ContinuousHopfieldNetwork(n_neurons=100)\n",
    "\n",
    "# Generate random patterns\n",
    "n_patterns = 5\n",
    "patterns = torch.randn(n_patterns, 100)\n",
    "patterns = torch.sign(patterns)  # Binarize patterns\n",
    "\n",
    "# Store patterns\n",
    "continuous_hopfield.store_patterns(patterns)\n",
    "\n",
    "print(f\"Stored {n_patterns} patterns in the continuous Hopfield network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test pattern recovery\n",
    "def add_noise(pattern, noise_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Add noise to pattern\n",
    "    \"\"\"\n",
    "    noisy_pattern = pattern.clone()\n",
    "    n_flips = int(noise_ratio * len(pattern))\n",
    "    flip_indices = np.random.choice(len(pattern), n_flips, replace=False)\n",
    "    noisy_pattern[flip_indices] *= -1\n",
    "    return noisy_pattern\n",
    "\n",
    "# Select a pattern for testing\n",
    "test_pattern = patterns[0]\n",
    "\n",
    "# Add noise\n",
    "noisy_pattern = add_noise(test_pattern, noise_ratio=0.3)\n",
    "\n",
    "# Recover pattern\n",
    "recovered_pattern, energies = continuous_hopfield.recover(noisy_pattern, n_steps=100)\n",
    "\n",
    "# Calculate similarity\n",
    "def similarity(p1, p2):\n",
    "    \"\"\"\n",
    "    Calculate similarity between two patterns\n",
    "    \"\"\"\n",
    "    return torch.dot(p1, p2).item() / len(p1)\n",
    "\n",
    "original_similarity = similarity(test_pattern, noisy_pattern)\n",
    "recovered_similarity = similarity(test_pattern, recovered_pattern)\n",
    "\n",
    "print(f\"Similarity between original and noisy pattern: {original_similarity:.4f}\")\n",
    "print(f\"Similarity between original and recovered pattern: {recovered_similarity:.4f}\")\n",
    "\n",
    "# Plot energy changes\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(energies)\n",
    "ax.set_xlabel('Step', fontsize=12)\n",
    "ax.set_ylabel('Energy', fontsize=12)\n",
    "ax.set_title('Energy Changes During Recovery', fontsize=14)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deep Boltzmann Machine\n",
    "\n",
    "Deep Boltzmann Machine (DBM) is a multi-layer extension of Restricted Boltzmann Machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepBoltzmannMachine:\n",
    "    \"\"\"\n",
    "    Deep Boltzmann Machine (DBM)\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_sizes, k=1, learning_rate=0.01, use_cuda=False):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.n_layers = len(layer_sizes) - 1\n",
    "        self.k = k\n",
    "        self.learning_rate = learning_rate\n",
    "        self.use_cuda = use_cuda\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        for i in range(self.n_layers):\n",
    "            # Initialize weights\n",
    "            W = torch.randn(layer_sizes[i], layer_sizes[i+1]) * 0.01\n",
    "            self.weights.append(W)\n",
    "            \n",
    "            # Initialize biases\n",
    "            if i == 0:  # Visible layer bias\n",
    "                b = torch.zeros(layer_sizes[i])\n",
    "            else:  # Hidden layer bias\n",
    "                b = torch.zeros(layer_sizes[i])\n",
    "            self.biases.append(b)\n",
    "            \n",
    "            # Next layer bias\n",
    "            if i == self.n_layers - 1:  # Last hidden layer bias\n",
    "                b = torch.zeros(layer_sizes[i+1])\n",
    "            else:  # Hidden layer bias\n",
    "                b = torch.zeros(layer_sizes[i+1])\n",
    "            self.biases.append(b)\n",
    "        \n",
    "        # Move to GPU if needed\n",
    "        if self.use_cuda:\n",
    "            for i in range(len(self.weights)):\n",
    "                self.weights[i] = self.weights[i].cuda()\n",
    "                self.biases[i] = self.biases[i].cuda()\n",
    "                self.biases[i+1] = self.biases[i+1].cuda()\n",
    "    \n",
    "    def sample_hidden(self, visible, layer_idx):\n",
    "        \"\"\"\n",
    "        Sample hidden layer given visible layer\n",
    "        \"\"\"\n",
    "        # Calculate activation probability\n",
    "        activation = torch.matmul(visible, self.weights[layer_idx]) + self.biases[layer_idx+1]\n",
    "        \n",
    "        # For the top layer, also consider the weight from the layer above\n",
    "        if layer_idx < self.n_layers - 1:\n",
    "            # This is a simplified implementation\n",
    "            # In practice, mean field approximation or other methods are needed\n",
    "            pass\n",
    "        \n",
    "        # Sample binary units\n",
    "        prob = torch.sigmoid(activation)\n",
    "        return torch.bernoulli(prob)\n",
    "    \n",
    "    def sample_visible(self, hidden, layer_idx):\n",
    "        \"\"\"\n",
    "        Sample visible layer given hidden layer\n",
    "        \"\"\"\n",
    "        # Calculate activation probability\n",
    "        activation = torch.matmul(hidden, self.weights[layer_idx].t()) + self.biases[layer_idx]\n",
    "        \n",
    "        # For the bottom layer, also consider the weight from the layer below\n",
    "        if layer_idx > 0:\n",
    "            # This is a simplified implementation\n",
    "            # In practice, mean field approximation or other methods are needed\n",
    "            pass\n",
    "        \n",
    "        # Sample binary units\n",
    "        prob = torch.sigmoid(activation)\n",
    "        return torch.bernoulli(prob)\n",
    "    \n",
    "    def contrastive_divergence(self, v0, k=None):\n",
    "        \"\"\"\n",
    "        Contrastive divergence algorithm\n",
    "        \"\"\"\n",
    "        if k is None:\n",
    "            k = self.k\n",
    "        \n",
    "        # Positive phase\n",
    "        ph_means, ph_samples = [], []\n",
    "        vk = v0\n",
    "        \n",
    "        # Bottom-up pass\n",
    "        for i in range(self.n_layers):\n",
    "            ph_mean = torch.sigmoid(torch.matmul(vk, self.weights[i]) + self.biases[i+1])\n",
    "            ph_sample = torch.bernoulli(ph_mean)\n",
    "            ph_means.append(ph_mean)\n",
    "            ph_samples.append(ph_sample)\n",
    "            vk = ph_sample\n",
    "        \n",
    "        # Negative phase\n",
    "        vk = v0.clone()\n",
    "        \n",
    "        # Gibbs sampling\n",
    "        for step in range(k):\n",
    "            # Bottom-up pass\n",
    "            for i in range(self.n_layers):\n",
    "                ph_mean = torch.sigmoid(torch.matmul(vk, self.weights[i]) + self.biases[i+1])\n",
    "                ph_sample = torch.bernoulli(ph_mean)\n",
    "                vk = ph_sample\n",
    "            \n",
    "            # Top-down pass (simplified)\n",
    "            for i in range(self.n_layers-1, -1, -1):\n",
    "                if i == 0:\n",
    "                    vk_mean = torch.sigmoid(torch.matmul(ph_samples[i], self.weights[i].t()) + self.biases[i])\n",
    "                    vk = torch.bernoulli(vk_mean)\n",
    "                else:\n",
    "                    # This is a simplified implementation\n",
    "                    pass\n",
    "        \n",
    "        # Update weights and biases\n",
    "        for i in range(self.n_layers):\n",
    "            # Update weights\n",
    "            dW = (torch.matmul(v0.t(), ph_means[i]) - torch.matmul(vk.t(), ph_samples[i])) / v0.size(0)\n",
    "            self.weights[i] += self.learning_rate * dW\n",
    "            \n",
    "            # Update biases\n",
    "            db = torch.mean(ph_means[i] - ph_samples[i], dim=0)\n",
    "            self.biases[i+1] += self.learning_rate * db\n",
    "            \n",
    "            if i == 0:\n",
    "                db = torch.mean(v0 - vk, dim=0)\n",
    "                self.biases[i] += self.learning_rate * db\n",
    "        \n",
    "        # Calculate reconstruction error\n",
    "        error = torch.mean((v0 - vk) ** 2)\n",
    "        \n",
    "        return error\n",
    "    \n",
    "    def train_batch(self, batch):\n",
    "        \"\"\"\n",
    "        Train one batch\n",
    "        \"\"\"\n",
    "        if self.use_cuda:\n",
    "            batch = batch.cuda()\n",
    "        \n",
    "        return self.contrastive_divergence(batch)\n",
    "    \n",
    "    def generate_samples(self, n_samples, n_gibbs_steps=100):\n",
    "        \"\"\"\n",
    "        Generate samples\n",
    "        \"\"\"\n",
    "        # Initialize random states\n",
    "        visible = torch.bernoulli(torch.ones(n_samples, self.layer_sizes[0]) * 0.5)\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            visible = visible.cuda()\n",
    "        \n",
    "        # Gibbs sampling\n",
    "        for _ in range(n_gibbs_steps):\n",
    "            # Bottom-up pass\n",
    "            for i in range(self.n_layers):\n",
    "                ph_mean = torch.sigmoid(torch.matmul(visible, self.weights[i]) + self.biases[i+1])\n",
    "                ph_sample = torch.bernoulli(ph_mean)\n",
    "                visible = ph_sample\n",
    "            \n",
    "            # Top-down pass (simplified)\n",
    "            for i in range(self.n_layers-1, -1, -1):\n",
    "                if i == 0:\n",
    "                    vk_mean = torch.sigmoid(torch.matmul(visible, self.weights[i].t()) + self.biases[i])\n",
    "                    visible = torch.bernoulli(vk_mean)\n",
    "                else:\n",
    "                    # This is a simplified implementation\n",
    "                    pass\n",
    "        \n",
    "        return visible\n",
    "\n",
    "# Create DBM\n",
    "layer_sizes = [784, 256, 128]  # Visible layer, first hidden layer, second hidden layer\n",
    "dbm = DeepBoltzmannMachine(layer_sizes, k=1, learning_rate=0.01, use_cuda=config.RBM_CONFIG['use_cuda'])\n",
    "\n",
    "print(f\"Created DBM with layers: {layer_sizes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "mnist_train_loader = load_mnist(batch_size=64, train=True)\n",
    "\n",
    "# Train DBM\n",
    "n_epochs = 5\n",
    "errors = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    batch_errors = []\n",
    "    \n",
    "    for batch_idx, (data, _) in enumerate(mnist_train_loader):\n",
    "        if batch_idx >= 20:  # Only train the first 20 batches as an example\n",
    "            break\n",
    "            \n",
    "        # Flatten data\n",
    "        batch = data.view(data.size(0), -1)\n",
    "        \n",
    "        # Binarize data\n",
    "        batch = (batch > 0.5).float()\n",
    "        \n",
    "        # Train one batch\n",
    "        error = dbm.train_batch(batch)\n",
    "        batch_errors.append(error)\n",
    "    \n",
    "    # Calculate average error\n",
    "    avg_error = np.mean([e.item() for e in batch_errors])\n",
    "    errors.append(avg_error)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}, Error: {avg_error:.6f}\")\n",
    "\n",
    "# Plot training error\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(errors)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Reconstruction Error', fontsize=12)\n",
    "ax.set_title('DBM Training Error', fontsize=14)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Conditional Generation\n",
    "\n",
    "Conditional generation can generate specific types of samples based on conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalRBM(RBM):\n",
    "    \"\"\"\n",
    "    Conditional Restricted Boltzmann Machine\n",
    "    \"\"\"\n",
    "    def __init__(self, n_visible, n_hidden, n_condition, k=1, learning_rate=0.01, use_cuda=False):\n",
    "        # Initialize parent class\n",
    "        super().__init__(n_visible, n_hidden, k, learning_rate, use_cuda)\n",
    "        \n",
    "        self.n_condition = n_condition\n",
    "        \n",
    "        # Initialize condition to hidden layer weights\n",
    "        self.W_cond = torch.randn(n_condition, n_hidden) * 0.01\n",
    "        \n",
    "        # Initialize condition bias\n",
    "        self.b_cond = torch.zeros(n_condition)\n",
    "        \n",
    "        # Move to GPU if needed\n",
    "        if self.use_cuda:\n",
    "            self.W_cond = self.W_cond.cuda()\n",
    "            self.b_cond = self.b_cond.cuda()\n",
    "    \n",
    "    def sample_h_given_v(self, v, condition=None):\n",
    "        \"\"\"\n",
    "        Sample hidden layer given visible layer and condition\n",
    "        \"\"\"\n",
    "        # Calculate activation probability\n",
    "        activation = torch.matmul(v, self.W) + self.b\n",
    "        \n",
    "        # Add condition influence\n",
    "        if condition is not None:\n",
    "            activation += torch.matmul(condition, self.W_cond)\n",
    "        \n",
    "        # Calculate probability\n",
    "        p_h = torch.sigmoid(activation)\n",
    "        \n",
    "        # Sample hidden layer\n",
    "        return torch.bernoulli(p_h), p_h\n",
    "    \n",
    "    def sample_v_given_h(self, h):\n",
    "        \"\"\"\n",
    "        Sample visible layer given hidden layer\n",
    "        \"\"\"\n",
    "        # Calculate activation probability\n",
    "        activation = torch.matmul(h, self.W.t()) + self.a\n",
    "        \n",
    "        # Calculate probability\n",
    "        p_v = torch.sigmoid(activation)\n",
    "        \n",
    "        # Sample visible layer\n",
    "        return torch.bernoulli(p_v), p_v\n",
    "    \n",
    "    def contrastive_divergence(self, v0, condition=None, k=None):\n",
    "        \"\"\"\n",
    "        Contrastive divergence algorithm\n",
    "        \"\"\"\n",
    "        if k is None:\n",
    "            k = self.k\n",
    "        \n",
    "        # Positive phase\n",
    "        ph, ph_mean = self.sample_h_given_v(v0, condition)\n",
    "        \n",
    "        # Negative phase\n",
    "        h = ph\n",
    "        for step in range(k):\n",
    "            vk, _ = self.sample_v_given_h(h)\n",
    "            h, _ = self.sample_h_given_v(vk, condition)\n",
    "        \n",
    "        vk, vk_mean = self.sample_v_given_h(h)\n",
    "        ph, ph_mean = self.sample_h_given_v(vk, condition)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        dW = (torch.matmul(v0.t(), ph_mean) - torch.matmul(vk.t(), ph_mean)) / v0.size(0)\n",
    "        self.W += self.learning_rate * dW\n",
    "        \n",
    "        da = torch.mean(v0 - vk, dim=0)\n",
    "        self.a += self.learning_rate * da\n",
    "        \n",
    "        db = torch.mean(ph_mean - ph, dim=0)\n",
    "        self.b += self.learning_rate * db\n",
    "        \n",
    "        # Update condition weights and biases\n",
    "        if condition is not None:\n",
    "            dW_cond = (torch.matmul(condition.t(), ph_mean) - torch.matmul(condition.t(), ph)) / v0.size(0)\n",
    "            self.W_cond += self.learning_rate * dW_cond\n",
    "            \n",
    "            db_cond = torch.mean(condition, dim=0)\n",
    "            self.b_cond += self.learning_rate * db_cond\n",
    "        \n",
    "        # Calculate reconstruction error\n",
    "        error = torch.mean((v0 - vk) ** 2)\n",
    "        \n",
    "        return error\n",
    "    \n",
    "    def generate_samples(self, n_samples, condition=None, n_gibbs_steps=100):\n",
    "        \"\"\"\n",
    "        Generate samples\n",
    "        \"\"\"\n",
    "        # Initialize random visible layer\n",
    "        v = torch.bernoulli(torch.ones(n_samples, self.n_visible) * 0.5)\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            v = v.cuda()\n",
    "            if condition is not None:\n",
    "                condition = condition.cuda()\n",
    "        \n",
    "        # Gibbs sampling\n",
    "        for _ in range(n_gibbs_steps):\n",
    "            _, h = self.sample_h_given_v(v, condition)\n",
    "            v, _ = self.sample_v_given_h(h)\n",
    "        \n",
    "        return v\n",
    "\n",
    "# Create conditional RBM\n",
    "n_visible = 784\n",
    "n_hidden = 256\n",
    "n_condition = 10  # 10 categories of MNIST\n",
    "\n",
    "conditional_rbm = ConditionalRBM(\n",
    "    n_visible=n_visible,\n",
    "    n_hidden=n_hidden,\n",
    "    n_condition=n_condition,\n",
    "    k=1,\n",
    "    learning_rate=0.01,\n",
    "    use_cuda=config.RBM_CONFIG['use_cuda']\n",
    ")\n",
    "\n",
    "print(f\"Created conditional RBM with {n_visible} visible units, {n_hidden} hidden units, and {n_condition} condition units\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare conditional data\n",
    "def prepare_conditional_data(data_loader, n_condition=10):\n",
    "    \"\"\"\n",
    "    Prepare conditional data\n",
    "    \"\"\"\n",
    "    conditional_data = []\n",
    "    \n",
    "    for data, labels in data_loader:\n",
    "        # Flatten data\n",
    "        batch = data.view(data.size(0), -1)\n",
    "        \n",
    "        # Binarize data\n",
    "        batch = (batch > 0.5).float()\n",
    "        \n",
    "        # Create one-hot encoding for labels\n",
    "        condition = torch.zeros(batch.size(0), n_condition)\n",
    "        for i, label in enumerate(labels):\n",
    "            condition[i, label] = 1.0\n",
    "        \n",
    "        conditional_data.append((batch, condition))\n",
    "    \n",
    "    return conditional_data\n",
    "\n",
    "# Load MNIST data\n",
    "mnist_train_loader = load_mnist(batch_size=64, train=True)\n",
    "conditional_train_data = prepare_conditional_data(mnist_train_loader)\n",
    "\n",
    "# Train conditional RBM\n",
    "n_epochs = 5\n",
    "errors = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    batch_errors = []\n",
    "    \n",
    "    for batch_idx, (batch, condition) in enumerate(conditional_train_data):\n",
    "        if batch_idx >= 20:  # Only train the first 20 batches as an example\n",
    "            break\n",
    "            \n",
    "        # Train one batch\n",
    "        error = conditional_rbm.contrastive_divergence(batch, condition)\n",
    "        batch_errors.append(error)\n",
    "    \n",
    "    # Calculate average error\n",
    "    avg_error = np.mean([e.item() for e in batch_errors])\n",
    "    errors.append(avg_error)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}, Error: {avg_error:.6f}\")\n",
    "\n",
    "# Plot training error\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(errors)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Reconstruction Error', fontsize=12)\n",
    "ax.set_title('Conditional RBM Training Error', fontsize=14)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate conditional samples\n",
    "def generate_conditional_samples(rbm, digit, n_samples=8, n_gibbs_steps=100):\n",
    "    \"\"\"\n",
    "    Generate samples for a specific digit\n",
    "    \"\"\"\n",
    "    # Create condition\n",
    "    condition = torch.zeros(n_samples, 10)\n",
    "    condition[:, digit] = 1.0\n",
    "    \n",
    "    # Generate samples\n",
    "    samples = rbm.generate_samples(n_samples, condition, n_gibbs_steps)\n",
    "    \n",
    "    return samples\n",
    "\n",
    "# Generate samples for each digit\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "\n",
    "for digit in range(10):\n",
    "    # Generate samples\n",
    "    samples = generate_conditional_samples(conditional_rbm, digit, n_samples=1)\n",
    "    \n",
    "    # Convert to image\n",
    "    image = binary_to_image(samples.detach().cpu().numpy(), (28, 28))[0]\n",
    "    \n",
    "    # Display\n",
    "    row = digit // 5\n",
    "    col = digit % 5\n",
    "    axes[row, col].imshow(image, cmap='binary')\n",
    "    axes[row, col].axis('off')\n",
    "    axes[row, col].set_title(f'Digit {digit}', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transfer Learning\n",
    "\n",
    "Transfer learning can apply a model trained on one dataset to another dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a basic RBM on MNIST\n",
    "mnist_train_loader = load_mnist(batch_size=64, train=True)\n",
    "\n",
    "rbm = RBM(\n",
    "    n_visible=784,\n",
    "    n_hidden=256,\n",
    "    k=1,\n",
    "    learning_rate=0.01,\n",
    "    use_cuda=config.RBM_CONFIG['use_cuda']\n",
    ")\n",
    "\n",
    "# Train RBM\n",
    "n_epochs = 5\n",
    "errors = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    batch_errors = []\n",
    "    \n",
    "    for batch_idx, (data, _) in enumerate(mnist_train_loader):\n",
    "        if batch_idx >= 20:  # Only train the first 20 batches as an example\n",
    "            break\n",
    "            \n",
    "        # Flatten data\n",
    "        batch = data.view(data.size(0), -1)\n",
    "        \n",
    "        # Binarize data\n",
    "        batch = (batch > 0.5).float()\n",
    "        \n",
    "        if rbm.use_cuda:\n",
    "            batch = batch.cuda()\n",
    "        \n",
    "        # Train one batch\n",
    "        error = rbm.train_batch(batch)\n",
    "        batch_errors.append(error)\n",
    "    \n",
    "    # Calculate average error\n",
    "    avg_error = np.mean([e.item() for e in batch_errors])\n",
    "    errors.append(avg_error)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}, Error: {avg_error:.6f}\")\n",
    "\n",
    "print(\"Basic RBM training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransferRBM(RBM):\n",
    "    \"\"\"\n",
    "    Transfer Learning RBM\n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained_rbm, freeze_hidden=False):\n",
    "        # Initialize new RBM\n",
    "        super().__init__(\n",
    "            n_visible=pretrained_rbm.n_visible,\n",
    "            n_hidden=pretrained_rbm.n_hidden,\n",
    "            k=pretrained_rbm.k,\n",
    "            learning_rate=pretrained_rbm.learning_rate,\n",
    "            momentum=pretrained_rbm.momentum,\n",
    "            weight_decay=pretrained_rbm.weight_decay,\n",
    "            use_cuda=pretrained_rbm.use_cuda\n",
    "        )\n",
    "        \n",
    "        # Copy pretrained weights\n",
    "        self.W.data = pretrained_rbm.W.data.clone()\n",
    "        self.b.data = pretrained_rbm.b.data.clone()\n",
    "        self.a.data = pretrained_rbm.a.data.clone()\n",
    "        \n",
    "        # Freeze hidden layer weights (optional)\n",
    "        if freeze_hidden:\n",
    "            self.W.requires_grad = False\n",
    "            self.b.requires_grad = False\n",
    "    \n",
    "    def fine_tune(self, data_loader, n_epochs=5, lr_factor=0.1):\n",
    "        \"\"\"\n",
    "        Fine-tune model\n",
    "        \"\"\"\n",
    "        # Lower learning rate\n",
    "        original_lr = self.learning_rate\n",
    "        self.learning_rate = original_lr * lr_factor\n",
    "        \n",
    "        errors = []\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            batch_errors = []\n",
    "            \n",
    "            for batch_idx, (data, _) in enumerate(data_loader):\n",
    "                # Flatten data\n",
    "                batch = data.view(data.size(0), -1)\n",
    "                \n",
    "                if self.use_cuda:\n",
    "                    batch = batch.cuda()\n",
    "                \n",
    "                # Train one batch\n",
    "                error = self.train_batch(batch)\n",
    "                batch_errors.append(error)\n",
    "            \n",
    "            # Calculate average error\n",
    "            avg_error = np.mean(batch_errors)\n",
    "            errors.append(avg_error)\n",
    "            \n",
    "            print(f\"Fine-tune Epoch {epoch+1}/{n_epochs}, Error: {avg_error:.6f}\")\n",
    "        \n",
    "        # Restore original learning rate\n",
    "        self.learning_rate = original_lr\n",
    "        \n",
    "        return errors\n",
    "\n",
    "# Use previously trained RBM for transfer learning\n",
    "transfer_rbm = TransferRBM(rbm, freeze_hidden=False)\n",
    "\n",
    "# Fine-tune\n",
    "fine_tune_errors = transfer_rbm.fine_tune(fashion_train_loader, n_epochs=5)\n",
    "\n",
    "print(\"Transfer learning completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare transfer learning vs training from scratch\n",
    "print(\"Comparing transfer learning vs training from scratch...\")\n",
    "\n",
    "# Train a new RBM from scratch\n",
    "scratch_rbm = RBM(\n",
    "    n_visible=784,\n",
    "    n_hidden=256,\n",
    "    k=1,\n",
    "    learning_rate=0.01,\n",
    "    use_cuda=config.RBM_CONFIG['use_cuda']\n",
    ")\n",
    "\n",
    "# Train new RBM\n",
    "scratch_errors = []\n",
    "n_epochs = 5\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    batch_errors = []\n",
    "    \n",
    "    for batch_idx, (data, _) in enumerate(fashion_train_loader):\n",
    "        if batch_idx >= 20:  # Only train the first 20 batches as an example\n",
    "            break\n",
    "            \n",
    "        # Flatten data\n",
    "        batch = data.view(data.size(0), -1)\n",
    "        \n",
    "        if scratch_rbm.use_cuda:\n",
    "            batch = batch.cuda()\n",
    "        \n",
    "        # Train one batch\n",
    "        error = scratch_rbm.train_batch(batch)\n",
    "        batch_errors.append(error)\n",
    "    \n",
    "    # Calculate average error\n",
    "    avg_error = np.mean(batch_errors)\n",
    "    scratch_errors.append(avg_error)\n",
    "    \n",
    "    print(f\"Scratch Epoch {epoch+1}/{n_epochs}, Error: {avg_error:.6f}\")\n",
    "\n",
    "# Plot comparison results\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(fine_tune_errors, 'b-o', label='Transfer Learning', linewidth=2)\n",
    "ax.plot(scratch_errors, 'r-s', label='Training from Scratch', linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Reconstruction Error', fontsize=12)\n",
    "ax.set_title('Transfer Learning vs Training from Scratch', fontsize=14)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Generate samples for comparison\n",
    "print(\"Generating samples for comparison...\")\n",
    "\n",
    "# Transfer learning RBM generates samples\n",
    "transfer_samples = transfer_rbm.generate_samples(16, n_gibbs_steps=100)\n",
    "transfer_images = binary_to_image(transfer_samples.detach().cpu().numpy(), (28, 28))\n",
    "\n",
    "# Training from scratch RBM generates samples\n",
    "scratch_samples = scratch_rbm.generate_samples(16, n_gibbs_steps=100)\n",
    "scratch_images = binary_to_image(scratch_samples.detach().cpu().numpy(), (28, 28))\n",
    "\n",
    "# Display samples\n",
    "fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "\n",
    "# Display transfer learning samples\n",
    "for i in range(8):\n",
    "    axes[0, i].imshow(transfer_images[i], cmap='binary')\n",
    "    axes[0, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[0, i].set_title('Transfer Learning', fontsize=12)\n",
    "\n",
    "# Display training from scratch samples\n",
    "for i in range(8):\n",
    "    axes[1, i].imshow(scratch_images[i], cmap='binary')\n",
    "    axes[1, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[1, i].set_title('Training from Scratch', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates some advanced experiments and extension features, including:\n",
    "\n",
    "1. **Continuous Hopfield Network**: Hopfield network using continuous dynamics equations\n",
    "2. **Deep Boltzmann Machine**: Multi-layer structure Boltzmann machine\n",
    "3. **Conditional Generation**: Generate specific types of samples based on conditions\n",
    "4. **Transfer Learning**: Apply a model trained on one dataset to another dataset\n",
    "\n",
    "These extensions demonstrate the flexibility and powerful functions of energy models and generative models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}