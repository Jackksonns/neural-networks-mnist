{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restricted Boltzmann Machine (RBM) Experiments\n",
    "\n",
    "This notebook showcases experimental results of Restricted Boltzmann Machines on the MNIST dataset, including training process, sample generation, weight analysis, and feature learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Add project root directory to path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import project modules\n",
    "from src.boltzmann.rbm import RBM\n",
    "from src.boltzmann.sampler import RBMSampler\n",
    "from src.boltzmann.experiments import RBMExperiments\n",
    "from src.utils.data_loader import MNISTLoader\n",
    "from src.utils.preprocessing import binary_to_image\n",
    "import config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loader\n",
    "loader = MNISTLoader()\n",
    "\n",
    "# Load MNIST data\n",
    "train_data = loader.get_train_data(binary_values={0, 1})\n",
    "test_data = loader.get_test_data(binary_values={0, 1})\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = data.DataLoader(train_data, batch_size=config.RBM_CONFIG['batch_size'], shuffle=True)\n",
    "test_loader = data.DataLoader(test_data, batch_size=config.RBM_CONFIG['batch_size'], shuffle=False)\n",
    "\n",
    "# Visualize some samples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, (data, labels) in enumerate(train_loader):\n",
    "    if i >= 2:  # Only show first two batches\n",
    "        break\n",
    "    for j in range(5):\n",
    "        if j >= len(data):\n",
    "            break\n",
    "        img = binary_to_image(data[j].numpy(), (28, 28))\n",
    "        axes[i, j].imshow(img, cmap='binary')\n",
    "        axes[i, j].set_title(f'Label: {labels[j].item()}')\n",
    "        axes[i, j].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RBM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RBM\n",
    "rbm = RBM(\n",
    "    n_visible=config.RBM_CONFIG['n_visible'],\n",
    "    n_hidden=config.RBM_CONFIG['n_hidden'],\n",
    "    k=config.RBM_CONFIG['k'],\n",
    "    learning_rate=config.RBM_CONFIG['learning_rate'],\n",
    "    momentum=config.RBM_CONFIG['momentum'],\n",
    "    weight_decay=config.RBM_CONFIG['weight_decay'],\n",
    "    use_cuda=config.RBM_CONFIG['use_cuda']\n",
    ")\n",
    "\n",
    "# Train RBM\n",
    "print(\"Training RBM...\")\n",
    "train_errors = []\n",
    "epoch_times = []\n",
    "n_epochs = 20\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    batch_errors = []\n",
    "    \n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        # Flatten data and binarize\n",
    "        batch = data.view(data.size(0), -1)\n",
    "        batch = (batch > 0.5).float()  # Binarize\n",
    "        \n",
    "        if rbm.use_cuda:\n",
    "            batch = batch.cuda()\n",
    "        \n",
    "        # Train a batch\n",
    "        error = rbm.train_batch(batch)\n",
    "        batch_errors.append(error)\n",
    "    \n",
    "    # Calculate average error\n",
    "    avg_error = np.mean(batch_errors)\n",
    "    train_errors.append(avg_error)\n",
    "    \n",
    "    # Record training time\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    epoch_times.append(epoch_time)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}, Error: {avg_error:.6f}, Time: {epoch_time:.2f}s\")\n",
    "\n",
    "print(\"RBM training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Process Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training process\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot training error\n",
    "ax1.plot(train_errors, 'b-', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Reconstruction Error', fontsize=12)\n",
    "ax1.set_title('Training Error', fontsize=14)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot training time\n",
    "ax2.plot(epoch_times, 'g-', linewidth=2)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Time (seconds)', fontsize=12)\n",
    "ax2.set_title('Training Time per Epoch', fontsize=14)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Weight Matrix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get weight matrix\n",
    "weights = rbm.get_weights()\n",
    "weights_np = weights.detach().cpu().numpy()\n",
    "\n",
    "# Visualize weight matrix heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(weights_np, cmap='coolwarm', center=0)\n",
    "plt.title('Weight Matrix', fontsize=14)\n",
    "plt.xlabel('Hidden Units', fontsize=12)\n",
    "plt.ylabel('Visible Units', fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "# Plot weight distribution histogram\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(weights_np.flatten(), bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.title('Weight Distribution', fontsize=14)\n",
    "plt.xlabel('Weight Value', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Visualize weight patterns corresponding to hidden units\n",
    "n_hidden = weights_np.shape[1]\n",
    "n_display = min(64, n_hidden)\n",
    "\n",
    "# Create figure\n",
    "grid_size = int(np.sqrt(n_display))\n",
    "fig, axes = plt.subplots(grid_size, grid_size, figsize=(12, 12))\n",
    "\n",
    "if grid_size == 1:\n",
    "    axes = np.array([[axes]])\n",
    "\n",
    "for i in range(grid_size):\n",
    "    for j in range(grid_size):\n",
    "        idx = i * grid_size + j\n",
    "        if idx < n_display:\n",
    "            # Reshape weights to image\n",
    "            weight_img = weights_np[:, idx].reshape(28, 28)\n",
    "            axes[i, j].imshow(weight_img, cmap='seismic')\n",
    "            axes[i, j].axis('off')\n",
    "            axes[i, j].set_title(f'Hidden {idx}', fontsize=8)\n",
    "        else:\n",
    "            axes[i, j].axis('off')\n",
    "\n",
    "# Set main title\n",
    "fig.suptitle('Hidden Unit Weight Patterns', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sample Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples\n",
    "print(\"Generating samples...\")\n",
    "n_samples = 64\n",
    "n_gibbs_steps = 1000\n",
    "\n",
    "samples = rbm.generate_samples(n_samples, n_gibbs_steps)\n",
    "\n",
    "# Convert samples to image format\n",
    "samples_np = samples.detach().cpu().numpy()\n",
    "images = binary_to_image(samples_np, (28, 28))\n",
    "\n",
    "# Create grid image\n",
    "grid_size = int(np.sqrt(n_samples))\n",
    "fig, axes = plt.subplots(grid_size, grid_size, figsize=(12, 12))\n",
    "\n",
    "for i in range(grid_size):\n",
    "    for j in range(grid_size):\n",
    "        idx = i * grid_size + j\n",
    "        if idx < len(images):\n",
    "            axes[i, j].imshow(images[idx], cmap='binary')\n",
    "            axes[i, j].axis('off')\n",
    "        else:\n",
    "            axes[i, j].axis('off')\n",
    "\n",
    "# Set main title\n",
    "fig.suptitle('Generated Samples', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Learning Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get hidden layer representations\n",
    "n_samples = 1000\n",
    "sample_loader = data.DataLoader(train_data, batch_size=n_samples, shuffle=True)\n",
    "\n",
    "# Get a batch of data\n",
    "for data, labels in sample_loader:\n",
    "    batch = data.view(data.size(0), -1)\n",
    "    batch = (batch > 0.5).float()  # Binarize\n",
    "    break\n",
    "\n",
    "# Get hidden layer representations\n",
    "hidden_repr = rbm.get_hidden_representation(batch)\n",
    "hidden_np = hidden_repr.detach().cpu().numpy()\n",
    "labels_np = labels.numpy()\n",
    "\n",
    "# Use t-SNE for dimensionality reduction\n",
    "print(\"Running t-SNE...\")\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "hidden_tsne = tsne.fit_transform(hidden_np)\n",
    "\n",
    "# Use PCA for dimensionality reduction\n",
    "pca = PCA(n_components=2)\n",
    "hidden_pca = pca.fit_transform(hidden_np)\n",
    "\n",
    "# Visualize dimensionality reduction results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot t-SNE results\n",
    "scatter1 = ax1.scatter(hidden_tsne[:, 0], hidden_tsne[:, 1], c=labels_np, cmap='tab10', alpha=0.7)\n",
    "ax1.set_xlabel('t-SNE Component 1', fontsize=12)\n",
    "ax1.set_ylabel('t-SNE Component 2', fontsize=12)\n",
    "ax1.set_title('t-SNE Visualization of Hidden Representations', fontsize=14)\n",
    "plt.colorbar(scatter1, ax=ax1)\n",
    "\n",
    "# Plot PCA results\n",
    "scatter2 = ax2.scatter(hidden_pca[:, 0], hidden_pca[:, 1], c=labels_np, cmap='tab10', alpha=0.7)\n",
    "ax2.set_xlabel('PCA Component 1', fontsize=12)\n",
    "ax2.set_ylabel('PCA Component 2', fontsize=12)\n",
    "ax2.set_title('PCA Visualization of Hidden Representations', fontsize=14)\n",
    "plt.colorbar(scatter2, ax=ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hidden Layer Activation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average hidden activations for each digit\n",
    "n_hidden = hidden_np.shape[1]\n",
    "n_digits = 10\n",
    "\n",
    "digit_activations = np.zeros((n_digits, n_hidden))\n",
    "digit_counts = np.zeros(n_digits)\n",
    "\n",
    "for i, label in enumerate(labels_np):\n",
    "    digit_activations[label] += hidden_np[i]\n",
    "    digit_counts[label] += 1\n",
    "\n",
    "# Calculate averages\n",
    "for digit in range(n_digits):\n",
    "    if digit_counts[digit] > 0:\n",
    "        digit_activations[digit] /= digit_counts[digit]\n",
    "\n",
    "# Create figure\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Plot heatmap\n",
    "sns.heatmap(digit_activations, cmap='viridis', ax=ax)\n",
    "ax.set_xlabel('Hidden Units', fontsize=12)\n",
    "ax.set_ylabel('Digits', fontsize=12)\n",
    "ax.set_title('Average Hidden Activations per Digit', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sampling Methods Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sampler\n",
    "sampler = RBMSampler(rbm)\n",
    "\n",
    "# Compare different sampling methods\n",
    "n_samples = 64\n",
    "n_steps = 1000\n",
    "\n",
    "# Standard Gibbs sampling\n",
    "print(\"Running Gibbs sampling...\")\n",
    "gibbs_samples, _ = sampler.gibbs_sample(n_samples, n_steps)\n",
    "\n",
    "# Block Gibbs sampling\n",
    "print(\"Running block Gibbs sampling...\")\n",
    "block_samples, _ = sampler.block_gibbs_sample(n_samples, n_steps)\n",
    "\n",
    "# Tempered transition sampling\n",
    "print(\"Running tempered transition sampling...\")\n",
    "annealed_samples, _ = sampler.tempered_transition_sample(n_samples, n_steps)\n",
    "\n",
    "# Parallel tempering sampling\n",
    "print(\"Running parallel tempering sampling...\")\n",
    "pt_samples, _ = sampler.parallel_tempering_sample(n_samples, n_steps)\n",
    "\n",
    "# Convert samples to image format\n",
    "gibbs_images = binary_to_image(gibbs_samples.detach().cpu().numpy(), (28, 28))\n",
    "block_images = binary_to_image(block_samples.detach().cpu().numpy(), (28, 28))\n",
    "annealed_images = binary_to_image(annealed_samples.detach().cpu().numpy(), (28, 28))\n",
    "pt_images = binary_to_image(pt_samples.detach().cpu().numpy(), (28, 28))\n",
    "\n",
    "# Select number of images to display\n",
    "n_display = 16\n",
    "\n",
    "# Create figure\n",
    "fig, axes = plt.subplots(4, n_display, figsize=(20, 8))\n",
    "\n",
    "# Display Gibbs sampling results\n",
    "for i in range(n_display):\n",
    "    if i < len(gibbs_images):\n",
    "        axes[0, i].imshow(gibbs_images[i], cmap='binary')\n",
    "    axes[0, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[0, i].set_title('Gibbs Sampling', fontsize=12)\n",
    "\n",
    "# Display block Gibbs sampling results\n",
    "for i in range(n_display):\n",
    "    if i < len(block_images):\n",
    "        axes[1, i].imshow(block_images[i], cmap='binary')\n",
    "    axes[1, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[1, i].set_title('Block Gibbs Sampling', fontsize=12)\n",
    "\n",
    "# Display tempered transition results\n",
    "for i in range(n_display):\n",
    "    if i < len(annealed_images):\n",
    "        axes[2, i].imshow(annealed_images[i], cmap='binary')\n",
    "    axes[2, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[2, i].set_title('Tempered Transition', fontsize=12)\n",
    "\n",
    "# Display parallel tempering results\n",
    "for i in range(n_display):\n",
    "    if i < len(pt_images):\n",
    "        axes[3, i].imshow(pt_images[i], cmap='binary')\n",
    "    axes[3, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[3, i].set_title('Parallel Tempering', fontsize=12)\n",
    "\n",
    "# Set main title\n",
    "fig.suptitle('Sampling Methods Comparison', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Dream Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run dream experiment\n",
    "digit = 0\n",
    "n_steps = 1000\n",
    "\n",
    "print(f\"Running dream experiment starting from digit {digit} for {n_steps} steps\")\n",
    "\n",
    "# Get sample of specified digit\n",
    "sample = loader.get_specific_digit_sample(digit, binary_values={0, 1})\n",
    "sample = sample.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Dream\n",
    "dream_sample, dream_history = sampler.dream(sample, n_steps)\n",
    "\n",
    "# Convert samples to image format\n",
    "original_image = binary_to_image(sample.detach().cpu().numpy(), (28, 28))[0]\n",
    "dream_image = binary_to_image(dream_sample.detach().cpu().numpy(), (28, 28))[0]\n",
    "\n",
    "# Convert dream history to images\n",
    "dream_images = []\n",
    "for state in dream_history:\n",
    "    img = binary_to_image(state.detach().cpu().numpy(), (28, 28))[0]\n",
    "    dream_images.append(img)\n",
    "\n",
    "# Select key frames to display\n",
    "n_frames = min(8, len(dream_images))\n",
    "frame_indices = np.linspace(0, len(dream_images)-1, n_frames, dtype=int)\n",
    "\n",
    "# Create figure\n",
    "fig, axes = plt.subplots(2, n_frames+1, figsize=(15, 6))\n",
    "\n",
    "# Display original image\n",
    "axes[0, 0].imshow(original_image, cmap='binary')\n",
    "axes[0, 0].set_title('Original', fontsize=12)\n",
    "axes[0, 0].axis('off')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "# Display key frames during dream process\n",
    "for i, idx in enumerate(frame_indices):\n",
    "    axes[0, i+1].imshow(dream_images[idx], cmap='binary')\n",
    "    axes[0, i+1].set_title(f'Step {idx}', fontsize=10)\n",
    "    axes[0, i+1].axis('off')\n",
    "    \n",
    "    # Calculate difference from original image\n",
    "    diff = np.abs(dream_images[idx] - original_image)\n",
    "    axes[1, i+1].imshow(diff, cmap='hot')\n",
    "    axes[1, i+1].set_title(f'Diff: {np.mean(diff):.3f}', fontsize=10)\n",
    "    axes[1, i+1].axis('off')\n",
    "\n",
    "# Set main title\n",
    "fig.suptitle('Dream Process', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Feature Extraction and Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hidden layer features from training and test sets\n",
    "print(\"Extracting features from training set...\")\n",
    "train_features = []\n",
    "train_labels = []\n",
    "\n",
    "for data, labels in train_loader:\n",
    "    batch = data.view(data.size(0), -1)\n",
    "    batch = (batch > 0.5).float()\n",
    "    \n",
    "    # Get hidden layer representation\n",
    "    hidden = rbm.get_hidden_representation(batch)\n",
    "    train_features.append(hidden)\n",
    "    train_labels.append(labels)\n",
    "\n",
    "train_features = torch.cat(train_features, dim=0).detach().cpu().numpy()\n",
    "train_labels = torch.cat(train_labels, dim=0).numpy()\n",
    "\n",
    "print(\"Extracting features from test set...\")\n",
    "test_features = []\n",
    "test_labels = []\n",
    "\n",
    "for data, labels in test_loader:\n",
    "    batch = data.view(data.size(0), -1)\n",
    "    batch = (batch > 0.5).float()\n",
    "    \n",
    "    # Get hidden layer representation\n",
    "    hidden = rbm.get_hidden_representation(batch)\n",
    "    test_features.append(hidden)\n",
    "    test_labels.append(labels)\n",
    "\n",
    "test_features = torch.cat(test_features, dim=0).detach().cpu().numpy()\n",
    "test_labels = torch.cat(test_labels, dim=0).numpy()\n",
    "\n",
    "print(f\"Training features shape: {train_features.shape}\")\n",
    "print(f\"Test features shape: {test_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use logistic regression classifier\n",
    "print(\"Training logistic regression on RBM features...\")\n",
    "lr_rbm = LogisticRegression(max_iter=1000)\n",
    "lr_rbm.fit(train_features, train_labels)\n",
    "rbm_pred = lr_rbm.predict(test_features)\n",
    "rbm_accuracy = accuracy_score(test_labels, rbm_pred)\n",
    "print(f\"Accuracy with RBM features: {rbm_accuracy:.4f}\")\n",
    "\n",
    "# Get raw pixel features for comparison\n",
    "print(\"Training logistic regression on raw pixels...\")\n",
    "train_pixels = train_data.data.view(train_data.data.size(0), -1).numpy() / 255.0\n",
    "test_pixels = test_data.data.view(test_data.data.size(0), -1).numpy() / 255.0\n",
    "\n",
    "lr_raw = LogisticRegression(max_iter=1000)\n",
    "lr_raw.fit(train_pixels, train_labels)\n",
    "raw_pred = lr_raw.predict(test_pixels)\n",
    "raw_accuracy = accuracy_score(test_labels, raw_pred)\n",
    "print(f\"Accuracy with raw pixels: {raw_accuracy:.4f}\")\n",
    "\n",
    "# Compare results\n",
    "print(f\"\\nImprovement: {rbm_accuracy - raw_accuracy:.4f}\")\n",
    "\n",
    "# Visualize comparison results\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "methods = ['Raw Pixels', 'RBM Features']\n",
    "accuracies = [raw_accuracy, rbm_accuracy]\n",
    "bars = ax.bar(methods, accuracies, color=['blue', 'green'])\n",
    "ax.set_ylabel('Accuracy', fontsize=12)\n",
    "ax.set_title('Classification Accuracy Comparison', fontsize=14)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# Add value labels\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "            f'{acc:.4f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook showcases various experimental results of RBM on the MNIST dataset, including:\n",
    "\n",
    "1. RBM training process and reconstruction error analysis\n",
    "2. Weight matrix analysis and visualization\n",
    "3. Sample generation and quality evaluation\n",
    "4. Feature learning and dimensionality reduction visualization\n",
    "5. Hidden layer activation pattern analysis\n",
    "6. Comparison of multiple sampling methods\n",
    "7. Dream experiment and process visualization\n",
    "8. Feature extraction and classification performance comparison\n",
    "\n",
    "These experiments help us understand the working principles, learning capabilities, and application potential of RBM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}